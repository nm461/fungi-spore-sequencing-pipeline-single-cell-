#!/bin/bash
#$ -N AN_Deep_referencemapping_Pipeline4
#$ -cwd
#$ -l h_rt=48:00:00
#$ -l s_vmem=4G,mem_req=4G
#$ -pe def_slot 8
#$ -o /home/nm461/referencemappingtesting/AN_deep/logs/AN_deep_referencemapping_Pipeline.out
#$ -e /home/nm461/referencemappingtesting/AN_deep/logs/AN_deep_referencemapping_Pipeline.err

set -e
set -x
set -o pipefail

# ============================================================
# CONFIG
# ============================================================

# Output root
MAP_ROOT="$HOME/referencemappingtesting/AN_deep/ReferenceMappingOut"

# Sample input root: contains AN_1 ... AN_16 directories
SAMPLES_ROOT="$HOME/AN_Deep"

# Logs root (must match SGE -o/-e above)
LOG_ROOT="$HOME/referencemappingtesting/AN_deep/logs"

# ------------------------------------------------------------
# REFERENCE (UNCOMPRESSED ONLY)
# ------------------------------------------------------------

# Source reference (must already be uncompressed .fna)
REF_SOURCE="$HOME/databases/reference_genomes/GCF_000002855.4_ASM285v2/GCF_000002855.4_ASM285v2_genomic.fna"
[ -f "$REF_SOURCE" ] || { echo "ERROR: reference not found: $REF_SOURCE"; exit 1; }

# Keep a project-local copy so bwa/samtools/picard indices stay together
REF_DIR="$MAP_ROOT/reference"
REF_FA="$REF_DIR/$(basename "$REF_SOURCE")"

# ------------------------------------------------------------
# Containers + tools
# ------------------------------------------------------------

SIF_DIR="$HOME/sif_images"
BWA_SIF="$SIF_DIR/bwa.sif"
SAMTOOLS_SIF="$SIF_DIR/samtools.sif"
MOSDEPTH_SIF="$SIF_DIR/mosdepth.sif"
PICARD_JAR="$HOME/tools/picard.jar"

# Java caps for Picard
JAVA_OPTS="-Xms1g -Xmx3g -XX:CompressedClassSpaceSize=128m -XX:MaxMetaspaceSize=512m -Djava.util.concurrent.ForkJoinPool.common.parallelism=4"

# Scratch (node-local) for samtools temp
SCRATCH_BASE="${TMPDIR:-$MAP_ROOT/tmp_scratch}"

# ============================================================
# ENV / MODULES
# ============================================================

module use /usr/local/package/modulefiles
module load apptainer

THREADS="${NSLOTS:-8}"

export _JAVA_OPTIONS="$JAVA_OPTS"
export JAVA_FORKJOINPOOL_COMMON_PARALLELISM=4
ulimit -u 4096 || true

# Make sure required directories exist
mkdir -p "$MAP_ROOT" "$REF_DIR" "$SCRATCH_BASE" "$LOG_ROOT"

# ------------------------------------------------------------
# STEP -1: Stage reference locally + build indexes (BWA + FAI)
# ------------------------------------------------------------

# Check containers exist BEFORE using them
[ -f "$BWA_SIF" ]      || { echo "ERROR: bwa SIF not found: $BWA_SIF"; exit 1; }
[ -f "$SAMTOOLS_SIF" ] || { echo "ERROR: samtools SIF not found: $SAMTOOLS_SIF"; exit 1; }

# Copy reference locally once (if not already copied)
if [ ! -f "$REF_FA" ]; then
  cp -f "$REF_SOURCE" "$REF_FA"
fi

# OPTIONAL: if old-style BWA index exists next to REF_SOURCE, copy it over
for ext in amb ann bwt pac sa; do
  if [ -f "${REF_SOURCE}.${ext}" ] && [ ! -f "${REF_FA}.${ext}" ]; then
    cp -f "${REF_SOURCE}.${ext}" "${REF_FA}.${ext}"
  fi
done

# OPTIONAL: if new-style BWA index exists next to REF_SOURCE, copy it over
for ext in 0123 0123.bwt 0123.pac 0123.ann 0123.amb 0123.sa; do
  if [ -f "${REF_SOURCE}.${ext}" ] && [ ! -f "${REF_FA}.${ext}" ]; then
    cp -f "${REF_SOURCE}.${ext}" "${REF_FA}.${ext}"
  fi
done

# samtools faidx (creates REF_FA.fai)
if [ ! -f "${REF_FA}.fai" ]; then
  echo "INFO: Building samtools faidx for $REF_FA"
  singularity exec "$SAMTOOLS_SIF" samtools faidx "$REF_FA"
fi

# bwa index (creates REF_FA.{amb,ann,bwt,pac,sa} OR REF_FA.0123*)
if [ ! -f "${REF_FA}.bwt" ] && [ ! -f "${REF_FA}.0123" ]; then
  echo "INFO: Building bwa index for $REF_FA"
  singularity exec "$BWA_SIF" bwa index "$REF_FA"
fi

echo "INFO: Reference staging/indexing done:"
echo "  REF_FA = $REF_FA"
ls -lh "$REF_FA" "${REF_FA}.fai" 2>/dev/null || true

# ============================================================
# CHECKS
# ============================================================

[ -f "$REF_FA" ] || { echo "ERROR: project reference copy missing: $REF_FA"; exit 1; }
# Ensure BWA index exists next to REF_FA
if [ ! -f "${REF_FA}.bwt" ] && [ ! -f "${REF_FA}.0123" ]; then
  echo "ERROR: BWA index files not found next to reference: ${REF_FA}.[amb|ann|bwt|pac|sa]"
  exit 1
fi

[ -f "$BWA_SIF" ] || { echo "ERROR: bwa SIF not found: $BWA_SIF"; exit 1; }
[ -f "$SAMTOOLS_SIF" ] || { echo "ERROR: samtools SIF not found: $SAMTOOLS_SIF"; exit 1; }
[ -f "$MOSDEPTH_SIF" ] || { echo "ERROR: mosdepth SIF not found: $MOSDEPTH_SIF"; exit 1; }
[ -f "$PICARD_JAR" ] || { echo "ERROR: picard jar not found: $PICARD_JAR"; exit 1; }

# ============================================================
# STEP 0: master CSV for refmap metrics
# ============================================================

MASTER_REFMAP_CSV="$MAP_ROOT/master_refmap_metrics.csv"
if [ ! -f "$MASTER_REFMAP_CSV" ]; then
  echo "Sample,MAPPED_PCT,PROPERLY_PAIRED_PCT,FRAC_COVERED_GT0,FRAC_COVERED_GE5,FRAC_COVERED_GE10,FRAC_ZERO_DEPTH,MEAN_DEPTH_ALL_POS,MEAN_DEPTH_COVERED_POS,DUP_READS" > "$MASTER_REFMAP_CSV"
fi

# ============================================================
# STEP 1: mapping + samtools QC + samtools markdup + summary
# Inputs:
#   $HOME/AN_Deep/AN_*/AN_*_R1.fastq.gz
#   $HOME/AN_Deep/AN_*/AN_*_R2.fastq.gz
# ============================================================

shopt -s nullglob

SAMPLE_DIRS=("$SAMPLES_ROOT"/AN_*)
if [ ${#SAMPLE_DIRS[@]} -eq 0 ]; then
  echo "ERROR: No sample directories found under $SAMPLES_ROOT matching AN_*"
  exit 1
fi

for SDIR in "${SAMPLE_DIRS[@]}"; do
  R1_LIST=("$SDIR"/*_R1.fastq.gz)
  if [ ${#R1_LIST[@]} -eq 0 ]; then
    echo "WARNING: No *_R1.fastq.gz found in $SDIR — skipping directory."
    continue
  fi

  for R1 in "${R1_LIST[@]}"; do
    SAMPLE=$(basename "$R1" _R1.fastq.gz)     # e.g. AN_1
    R2="$SDIR/${SAMPLE}_R2.fastq.gz"

    if [ ! -f "$R2" ]; then
      echo "WARNING: missing R2 for $SAMPLE ($R2) — skipping."
      continue
    fi

    OUT_DIR="$MAP_ROOT/$SAMPLE"
    mkdir -p "$OUT_DIR"

    LOG="$OUT_DIR/run.step1_mapping.log"
    echo "===== $(date) START STEP1 $SAMPLE =====" | tee -a "$LOG"
    echo "R1=$R1" | tee -a "$LOG"
    echo "R2=$R2" | tee -a "$LOG"
    echo "OUT_DIR=$OUT_DIR" | tee -a "$LOG"
    echo "THREADS=$THREADS" | tee -a "$LOG"
    echo "REF_FA=$REF_FA" | tee -a "$LOG"

    SCRATCH="$SCRATCH_BASE/$SAMPLE"
    mkdir -p "$SCRATCH"

    # Idempotent skip (FIX): only skip if BAM+index exist (metrics can exist even when BAM failed)
    if [ -f "$OUT_DIR/refmap.sorted.bam" ] && [ -f "$OUT_DIR/refmap.sorted.bam.bai" ]; then
      echo "INFO: $SAMPLE already has refmap.sorted.bam — skipping STEP1." | tee -a "$LOG"
      continue
    fi

    if ! (
      SORT_TMP="$SCRATCH/${SAMPLE}.refmap.sorted.bam"
      TMP_PREFIX="$SCRATCH/${SAMPLE}.sorttmp"

      singularity exec -B "$SCRATCH:$SCRATCH" -B "$OUT_DIR:$OUT_DIR" "$BWA_SIF" bwa mem -t "$THREADS" \
        "$REF_FA" "$R1" "$R2" \
        2> "$OUT_DIR/bwa_mem.stderr.log" \
      | singularity exec -B "$SCRATCH:$SCRATCH" -B "$OUT_DIR:$OUT_DIR" "$SAMTOOLS_SIF" samtools view -b - \
      | singularity exec -B "$SCRATCH:$SCRATCH" -B "$OUT_DIR:$OUT_DIR" "$SAMTOOLS_SIF" samtools sort -@ "$THREADS" \
          -T "$TMP_PREFIX" \
          -o "$SORT_TMP" -

      cp -f "$SORT_TMP" "$OUT_DIR/refmap.sorted.bam"
      rm -f "$SORT_TMP"

      singularity exec -B "$OUT_DIR:$OUT_DIR" "$SAMTOOLS_SIF" samtools index "$OUT_DIR/refmap.sorted.bam"

      singularity exec -B "$OUT_DIR:$OUT_DIR" "$SAMTOOLS_SIF" samtools flagstat \
        "$OUT_DIR/refmap.sorted.bam" > "$OUT_DIR/refmap.flagstat.txt"

      singularity exec -B "$OUT_DIR:$OUT_DIR" "$SAMTOOLS_SIF" samtools depth -a \
        "$OUT_DIR/refmap.sorted.bam" > "$OUT_DIR/refmap.depth.txt"

      # samtools markdup pipeline
      NAME_BAM="$SCRATCH/${SAMPLE}.name.bam"
      FIXMATE_BAM="$SCRATCH/${SAMPLE}.fixmate.bam"
      FIXSORT_BAM="$SCRATCH/${SAMPLE}.fixmate.sorted.bam"
      MD_BAM_TMP="$SCRATCH/${SAMPLE}.markdup.bam"

      singularity exec -B "$SCRATCH:$SCRATCH" -B "$OUT_DIR:$OUT_DIR" "$SAMTOOLS_SIF" samtools sort -n -@ "$THREADS" \
        -T "$TMP_PREFIX.name" \
        -o "$NAME_BAM" "$OUT_DIR/refmap.sorted.bam"

      singularity exec -B "$SCRATCH:$SCRATCH" -B "$OUT_DIR:$OUT_DIR" "$SAMTOOLS_SIF" samtools fixmate -m \
        "$NAME_BAM" "$FIXMATE_BAM"

      singularity exec -B "$SCRATCH:$SCRATCH" -B "$OUT_DIR:$OUT_DIR" "$SAMTOOLS_SIF" samtools sort -@ "$THREADS" \
        -T "$TMP_PREFIX.fix" \
        -o "$FIXSORT_BAM" "$FIXMATE_BAM"

      singularity exec -B "$SCRATCH:$SCRATCH" -B "$OUT_DIR:$OUT_DIR" "$SAMTOOLS_SIF" samtools markdup -@ "$THREADS" -s \
        "$FIXSORT_BAM" "$MD_BAM_TMP" \
        2> "$OUT_DIR/markdup.metrics.txt"

      cp -f "$MD_BAM_TMP" "$OUT_DIR/refmap.markdup.bam"
      rm -f "$MD_BAM_TMP"

      singularity exec -B "$OUT_DIR:$OUT_DIR" "$SAMTOOLS_SIF" samtools index "$OUT_DIR/refmap.markdup.bam"

      singularity exec -B "$OUT_DIR:$OUT_DIR" "$SAMTOOLS_SIF" samtools flagstat \
        "$OUT_DIR/refmap.markdup.bam" > "$OUT_DIR/refmap.markdup.flagstat.txt"

      # metrics summary
      FLAGSTAT="$OUT_DIR/refmap.flagstat.txt"
      DEPTH="$OUT_DIR/refmap.depth.txt"
      OUT_TSV="$OUT_DIR/refmap.metrics_summary.tsv"
      OUT_CSV="$OUT_DIR/refmap.metrics_summary.csv"

      TOTAL_READS=$(awk '/in total/ {print $1; exit}' "$FLAGSTAT")
      MAPPED_READS=$(awk '/ mapped \(/ && !/primary/ {print $1; exit}' "$FLAGSTAT")
      MAPPED_PCT=$(awk '/ mapped \(/ && !/primary/ {match($0,/\(([0-9.]+)%/,a); print (a[1]=="" ? "NA" : a[1]); exit}' "$FLAGSTAT")

      PAIRED=$(awk '/ paired in sequencing/ {print $1; exit}' "$FLAGSTAT")
      PROPER_PAIRED=$(awk '/ properly paired/ {print $1; exit}' "$FLAGSTAT")
      PROPER_PAIRED_PCT=$(awk '/ properly paired/ {match($0,/\(([0-9.]+)%/,a); print (a[1]=="" ? "NA" : a[1]); exit}' "$FLAGSTAT")

      SINGLETONS=$(awk '/ singletons/ {print $1; exit}' "$FLAGSTAT")
      MATE_DIFF_CHR=$(awk '/with mate mapped to a different chr$/ {print $1; exit}' "$FLAGSTAT")
      MATE_DIFF_CHR_MQ5=$(awk '/with mate mapped to a different chr \(mapQ>=5\)/ {print $1; exit}' "$FLAGSTAT")

      read -r POSITIONS MEAN_DEPTH_ALL MEAN_DEPTH_COV \
              COVER_GT0 FRAC_GT0 \
              COVER_GE5 FRAC_GE5 \
              COVER_GE10 FRAC_GE10 \
              ZERO_COUNT ZERO_FRAC <<EOF
$(awk '{
          d=$3; t++; sum+=d;
          if(d==0) z++;
          if(d>0){c0++; sumcov+=d}
          if(d>=5) c5++;
          if(d>=10) c10++;
        }
        END{
          printf("%d %.6f %.6f %d %.6f %d %.6f %d %.6f %d %.6f\n",
                 t, sum/t, (c0?sumcov/c0:0),
                 c0, c0/t,
                 c5, c5/t,
                 c10, c10/t,
                 z, z/t);
        }' "$DEPTH")
EOF

      DUP_READS=$(awk '/ duplicates/ && !/primary/ {print $1; exit}' "$OUT_DIR/refmap.markdup.flagstat.txt" || true)

      {
        echo -e "Metric\tValue"
        echo -e "SAMPLE\t$SAMPLE"
        echo -e "MAP_DIR\t$OUT_DIR"
        echo -e "BAM\t$OUT_DIR/refmap.sorted.bam"
        echo -e "FLAGSTAT\t$FLAGSTAT"
        echo -e "DEPTH\t$DEPTH"
        echo -e "TOTAL_READS\t$TOTAL_READS"
        echo -e "MAPPED_READS\t$MAPPED_READS"
        echo -e "MAPPED_PCT\t$MAPPED_PCT"
        echo -e "PAIRED_IN_SEQUENCING\t$PAIRED"
        echo -e "PROPERLY_PAIRED\t$PROPER_PAIRED"
        echo -e "PROPERLY_PAIRED_PCT\t$PROPER_PAIRED_PCT"
        echo -e "SINGLETONS\t$SINGLETONS"
        echo -e "MATE_DIFF_CHR\t$MATE_DIFF_CHR"
        echo -e "MATE_DIFF_CHR_MAPQ_GE5\t$MATE_DIFF_CHR_MQ5"
        echo -e "POSITIONS_IN_DEPTH\t$POSITIONS"
        echo -e "MEAN_DEPTH_ALL_POS\t$MEAN_DEPTH_ALL"
        echo -e "MEAN_DEPTH_COVERED_POS\t$MEAN_DEPTH_COV"
        echo -e "FRAC_COVERED_GT0\t$FRAC_GT0"
        echo -e "FRAC_COVERED_GE5\t$FRAC_GE5"
        echo -e "FRAC_COVERED_GE10\t$FRAC_GE10"
        echo -e "FRAC_ZERO_DEPTH\t$ZERO_FRAC"
        echo -e "DUP_READS_MARKDUP_BAM\t$DUP_READS"
      } > "$OUT_TSV"

      awk -F'\t' 'BEGIN{OFS=","} NR==1{print "Metric","Value";next} {print $1,$2}' "$OUT_TSV" > "$OUT_CSV"

      echo "$SAMPLE,$MAPPED_PCT,$PROPER_PAIRED_PCT,$FRAC_GT0,$FRAC_GE5,$FRAC_GE10,$ZERO_FRAC,$MEAN_DEPTH_ALL,$MEAN_DEPTH_COV,$DUP_READS" \
        >> "$MASTER_REFMAP_CSV"
    ) >> "$LOG" 2>&1; then
      echo "ERROR processing STEP1 $SAMPLE — see $LOG. Continuing." | tee -a "$LOG"
      continue
    fi

    echo "===== $(date) DONE STEP1 $SAMPLE =====" | tee -a "$LOG"
  done
done

echo "STEP1 complete. Master table: $MASTER_REFMAP_CSV"
# ============================================================
# STEP 2: Picard + mosdepth
# ============================================================

REF_FAI="${REF_FA}.fai"
REF_DICT="${REF_FA%.fna}.dict"   # safest for .fna

if [ ! -f "$REF_FAI" ]; then
  singularity exec "$SAMTOOLS_SIF" samtools faidx "$REF_FA"
fi

if [ ! -f "$REF_DICT" ]; then
  java -jar "$PICARD_JAR" CreateSequenceDictionary R="$REF_FA" O="$REF_DICT"
fi

BIN_10KB="$MAP_ROOT/bins_10kb.bed"
BIN_50KB="$MAP_ROOT/bins_50kb.bed"

if [ ! -s "$BIN_10KB" ]; then
  awk 'BEGIN{OFS="\t"} {for(i=0;i<$2;i+=10000){e=i+10000; if(e>$2)e=$2; print $1,i,e}}' \
    "$REF_FAI" > "$BIN_10KB"
fi

if [ ! -s "$BIN_50KB" ]; then
  awk 'BEGIN{OFS="\t"} {for(i=0;i<$2;i+=50000){e=i+50000; if(e>$2)e=$2; print $1,i,e}}' \
    "$REF_FAI" > "$BIN_50KB"
fi

shopt -s nullglob
BAM_LIST=("$MAP_ROOT"/*/refmap.sorted.bam)
if [ ${#BAM_LIST[@]} -eq 0 ]; then
  echo "ERROR: No refmap.sorted.bam found under $MAP_ROOT"
  exit 1
fi

for BAM in "${BAM_LIST[@]}"; do
  SAMPLE=$(basename "$(dirname "$BAM")")
  OUT_DIR="$(dirname "$BAM")"
  LOG="$OUT_DIR/run.step2_picard_mosdepth.log"

  echo "===== $(date) START STEP2 $SAMPLE =====" | tee -a "$LOG"
  echo "BAM=$BAM" | tee -a "$LOG"
  echo "THREADS=$THREADS" | tee -a "$LOG"

  # Idempotent skip: if mosdepth regions exist, assume step2 done
  if [ -f "$OUT_DIR/mosdepth_10kb.regions.bed.gz" ] && [ -f "$OUT_DIR/mosdepth_50kb.regions.bed.gz" ]; then
    echo "INFO: $SAMPLE already has mosdepth regions 10kb+50kb — skipping STEP2." | tee -a "$LOG"
    continue
  fi

  if ! (
    if [ ! -f "${BAM}.bai" ]; then
      singularity exec "$SAMTOOLS_SIF" samtools index "$BAM"
    fi

    RGBAM="$OUT_DIR/refmap.rg.bam"
    if [ ! -f "$RGBAM" ]; then
      java -jar "$PICARD_JAR" AddOrReplaceReadGroups \
        I="$BAM" \
        O="$RGBAM" \
        RGID="$SAMPLE" \
        RGLB="lib1" \
        RGPL="ILLUMINA" \
        RGPU="${SAMPLE}.unit1" \
        RGSM="$SAMPLE" \
        SORT_ORDER=coordinate \
        CREATE_INDEX=true \
        VALIDATION_STRINGENCY=SILENT
    fi

    MDBAM="$OUT_DIR/refmap.picard_md.bam"
    MD_METRICS="$OUT_DIR/picard.MarkDuplicates.metrics.txt"

    if [ ! -f "$MDBAM" ]; then
      java -jar "$PICARD_JAR" MarkDuplicates \
        I="$RGBAM" \
        O="$MDBAM" \
        M="$MD_METRICS" \
        CREATE_INDEX=true \
        ASSUME_SORT_ORDER=coordinate \
        VALIDATION_STRINGENCY=SILENT \
        READ_NAME_REGEX=null
    fi

    java -jar "$PICARD_JAR" CollectGcBiasMetrics \
      I="$MDBAM" \
      O="$OUT_DIR/picard.gc_bias_metrics.txt" \
      CHART="$OUT_DIR/picard.gc_bias.pdf" \
      S="$OUT_DIR/picard.gc_bias_summary.txt" \
      R="$REF_FA"

    java -jar "$PICARD_JAR" CollectWgsMetrics \
      I="$MDBAM" \
      O="$OUT_DIR/picard.wgs_metrics.txt" \
      R="$REF_FA"

    singularity exec "$MOSDEPTH_SIF" mosdepth \
      --threads "$THREADS" \
      --by "$BIN_10KB" \
      "$OUT_DIR/mosdepth_10kb" \
      "$MDBAM"

    singularity exec "$MOSDEPTH_SIF" mosdepth \
      --threads "$THREADS" \
      --by "$BIN_50KB" \
      "$OUT_DIR/mosdepth_50kb" \
      "$MDBAM"
  ) >> "$LOG" 2>&1; then
    echo "ERROR processing STEP2 $SAMPLE — see $LOG. Continuing." | tee -a "$LOG"
    continue
  fi

  echo "===== $(date) DONE STEP2 $SAMPLE =====" | tee -a "$LOG"
done

echo "STEP2 complete."

# ============================================================
# STEP 3: Extract mosdepth bin QC metrics (10kb + 50kb)
# ============================================================

export MAP_ROOT
export REF_FA
export REF_FAI

python3 - <<'PY'
import os, gzip, math, csv
from pathlib import Path
from statistics import median

MAP_ROOT = Path(os.environ["MAP_ROOT"])
REF_FA   = Path(os.environ["REF_FA"])

BIN_LABELS = ["10kb", "50kb"]
REGIONS_SUFFIX = {b: f"mosdepth_{b}.regions.bed.gz" for b in BIN_LABELS}
THRESHOLDS = [0.1, 1, 2, 5, 10, 20]

def load_fasta_as_dict(fa_path: Path):
    seqs = {}
    name = None
    chunks = []
    with fa_path.open("rt") as f:
        for line in f:
            line = line.rstrip("\n")
            if not line:
                continue
            if line.startswith(">"):
                if name is not None:
                    seqs[name] = "".join(chunks).upper()
                name = line[1:].split()[0]
                chunks = []
            else:
                chunks.append(line.strip())
        if name is not None:
            seqs[name] = "".join(chunks).upper()
    return seqs

def gc_fraction(seq: str) -> float:
    if not seq:
        return float("nan")
    seq = seq.upper()
    g = seq.count("G"); c = seq.count("C")
    a = seq.count("A"); t = seq.count("T")
    denom = a + t + g + c
    if denom == 0:
        return float("nan")
    return (g + c) / denom

def quantile_sorted(xs, q):
    n = len(xs)
    if n == 0:
        return float("nan")
    if n == 1:
        return xs[0]
    pos = (n - 1) * q
    lo = int(math.floor(pos))
    hi = int(math.ceil(pos))
    if lo == hi:
        return xs[lo]
    frac = pos - lo
    return xs[lo] * (1 - frac) + xs[hi] * frac

def mad(xs):
    if not xs:
        return float("nan")
    m = median(xs)
    return median([abs(x - m) for x in xs])

def gini(values):
    vals = [v for v in values if v is not None and not math.isnan(v)]
    if len(vals) == 0:
        return float("nan")
    if all(v == 0 for v in vals):
        return 0.0
    vals = sorted(vals)
    n = len(vals)
    s = sum(vals)
    if s == 0:
        return 0.0
    cum = 0.0
    for i, x in enumerate(vals, start=1):
        cum += i * x
    return (2.0 * cum) / (n * s) - (n + 1.0) / n

def rankdata(a):
    n = len(a)
    order = sorted(range(n), key=lambda i: a[i])
    ranks = [0.0] * n
    i = 0
    r = 1
    while i < n:
        j = i
        while j + 1 < n and a[order[j+1]] == a[order[i]]:
            j += 1
        avg = (r + (r + (j - i))) / 2.0
        for k in range(i, j + 1):
            ranks[order[k]] = avg
        r += (j - i + 1)
        i = j + 1
    return ranks

def pearson(x, y):
    n = len(x)
    if n < 3:
        return float("nan")
    mx = sum(x) / n
    my = sum(y) / n
    vx = sum((xi - mx) ** 2 for xi in x)
    vy = sum((yi - my) ** 2 for yi in y)
    if vx == 0 or vy == 0:
        return float("nan")
    cov = sum((xi - mx) * (yi - my) for xi, yi in zip(x, y))
    return cov / math.sqrt(vx * vy)

def spearman(x, y):
    if len(x) < 3:
        return float("nan")
    return pearson(rankdata(x), rankdata(y))

def top_share(vals, pct):
    if not vals:
        return float("nan")
    s = sum(vals)
    if s == 0:
        return 0.0
    vals_sorted = sorted(vals, reverse=True)
    k = max(1, int(math.ceil(len(vals_sorted) * pct)))
    return sum(vals_sorted[:k]) / s

def mapd_like(depths_by_contig, eps=1e-3):
    diffs = []
    for ds in depths_by_contig.values():
        prev = None
        for d in ds:
            if d <= 0:
                prev = None
                continue
            v = math.log(d + eps, 2)
            if prev is not None:
                diffs.append(abs(v - prev))
            prev = v
    return median(diffs) if diffs else float("nan")

print(f"Loading reference FASTA: {REF_FA}")
ref = load_fasta_as_dict(REF_FA)
print(f"Loaded contigs: {len(ref)}")

sample_dirs = sorted([p for p in MAP_ROOT.iterdir() if p.is_dir()])
if not sample_dirs:
    raise SystemExit(f"ERROR: no sample dirs found under {MAP_ROOT}")

for bin_label in BIN_LABELS:
    master_rows = []
    master_csv = MAP_ROOT / f"master_bin_qc_metrics_{bin_label}.csv"

    master_header = [
        "Sample","BinLabel","BinsTotal","GC_valid_frac","DropoutFrac_depth0",
        "MeanDepth_all","MedianDepth_all","MeanDepth_nonzero","MedianDepth_nonzero",
        "IQR_nonzero","MAD_nonzero","CV_nonzero","Gini_nonzero",
        "Top1pct_share_nonzero","Top5pct_share_nonzero","Top10pct_share_nonzero",
        "MAPD_like_nonzero","GC_Spearman_nonzero","GC_Pearson_nonzero",
    ] + [f"Breadth_ge{t}".replace(".","p") for t in THRESHOLDS]

    for sd in sample_dirs:
        sample = sd.name
        regions_path = sd / REGIONS_SUFFIX[bin_label]
        if not regions_path.exists():
            print(f"WARNING: missing {regions_path} — skipping {sample} for {bin_label}")
            continue

        out_bins = sd / f"bin_qc_{bin_label}.tsv.gz"
        summary_tsv = sd / f"bin_qc_summary_{bin_label}.tsv"

        bins_total = 0
        depth0 = 0
        ge_counts = {t: 0 for t in THRESHOLDS}
        gc_valid = 0

        depths_all = []
        depths_nz = []
        gc_nz = []
        depth_nz_for_gc = []
        depths_by_contig = {}

        with gzip.open(regions_path, "rt") as f_in, gzip.open(out_bins, "wt") as f_out:
            f_out.write("contig\tstart\tend\tlen\tmean_depth\tgc_frac\n")
            for line in f_in:
                line = line.strip()
                if not line:
                    continue
                parts = line.split("\t")
                if len(parts) < 4:
                    continue
                contig, start_s, end_s, depth_s = parts[0], parts[1], parts[2], parts[3]
                start = int(start_s); end = int(end_s); d = float(depth_s)

                bins_total += 1
                depths_all.append(d)
                depths_by_contig.setdefault(contig, []).append(d)

                if d == 0:
                    depth0 += 1
                for t in THRESHOLDS:
                    if d >= t:
                        ge_counts[t] += 1

                seq = ref.get(contig, "")
                if seq and end <= len(seq):
                    gc = gc_fraction(seq[start:end])
                else:
                    gc = float("nan")

                if not math.isnan(gc):
                    gc_valid += 1
                if d > 0:
                    depths_nz.append(d)
                    if not math.isnan(gc):
                        depth_nz_for_gc.append(d)
                        gc_nz.append(gc)

                f_out.write(f"{contig}\t{start}\t{end}\t{end-start}\t{d:.6f}\t{gc if not math.isnan(gc) else 'NA'}\n")

        if bins_total == 0:
            continue

        gc_valid_frac = gc_valid / bins_total
        dropout_frac = depth0 / bins_total

        mean_all = sum(depths_all) / bins_total
        med_all = quantile_sorted(sorted(depths_all), 0.5)
        breadths = {t: ge_counts[t] / bins_total for t in THRESHOLDS}

        if depths_nz:
            mean_nz = sum(depths_nz) / len(depths_nz)
            nz_sorted = sorted(depths_nz)
            med_nz = quantile_sorted(nz_sorted, 0.5)
            q1 = quantile_sorted(nz_sorted, 0.25)
            q3 = quantile_sorted(nz_sorted, 0.75)
            iqr_nz = q3 - q1
            mad_nz = mad(depths_nz)

            if len(depths_nz) >= 2 and mean_nz > 0:
                var = sum((x - mean_nz) ** 2 for x in depths_nz) / (len(depths_nz) - 1)
                cv_nz = math.sqrt(var) / mean_nz
            else:
                cv_nz = float("nan")

            gini_nz = gini(depths_nz)
            top1 = top_share(depths_nz, 0.01)
            top5 = top_share(depths_nz, 0.05)
            top10 = top_share(depths_nz, 0.10)
            mapd = mapd_like(depths_by_contig)
        else:
            mean_nz = med_nz = iqr_nz = mad_nz = cv_nz = gini_nz = top1 = top5 = top10 = mapd = float("nan")

        if len(gc_nz) >= 3:
            gc_spear = spearman(gc_nz, depth_nz_for_gc)
            gc_pear = pearson(gc_nz, depth_nz_for_gc)
        else:
            gc_spear = float("nan"); gc_pear = float("nan")

        with summary_tsv.open("wt") as s:
            s.write("Metric\tValue\n")
            s.write(f"Sample\t{sample}\n")
            s.write(f"BinLabel\t{bin_label}\n")
            s.write(f"BinsTotal\t{bins_total}\n")
            s.write(f"GC_valid_frac\t{gc_valid_frac:.6f}\n")
            s.write(f"DropoutFrac_depth0\t{dropout_frac:.6f}\n")
            for t in THRESHOLDS:
                s.write(f"{('Breadth_ge'+str(t)).replace('.','p')}\t{breadths[t]:.6f}\n")
            s.write(f"MeanDepth_all\t{mean_all:.6f}\n")
            s.write(f"MedianDepth_all\t{med_all:.6f}\n")
            s.write(f"MeanDepth_nonzero\t{mean_nz:.6f}\n" if not math.isnan(mean_nz) else "MeanDepth_nonzero\tNA\n")
            s.write(f"MedianDepth_nonzero\t{med_nz:.6f}\n" if not math.isnan(med_nz) else "MedianDepth_nonzero\tNA\n")
            s.write(f"IQR_nonzero\t{iqr_nz:.6f}\n" if not math.isnan(iqr_nz) else "IQR_nonzero\tNA\n")
            s.write(f"MAD_nonzero\t{mad_nz:.6f}\n" if not math.isnan(mad_nz) else "MAD_nonzero\tNA\n")
            s.write(f"CV_nonzero\t{cv_nz:.6f}\n" if not math.isnan(cv_nz) else "CV_nonzero\tNA\n")
            s.write(f"Gini_nonzero\t{gini_nz:.6f}\n" if not math.isnan(gini_nz) else "Gini_nonzero\tNA\n")
            s.write(f"Top1pct_share_nonzero\t{top1:.6f}\n" if not math.isnan(top1) else "Top1pct_share_nonzero\tNA\n")
            s.write(f"Top5pct_share_nonzero\t{top5:.6f}\n" if not math.isnan(top5) else "Top5pct_share_nonzero\tNA\n")
            s.write(f"Top10pct_share_nonzero\t{top10:.6f}\n" if not math.isnan(top10) else "Top10pct_share_nonzero\tNA\n")
            s.write(f"MAPD_like_nonzero\t{mapd:.6f}\n" if not math.isnan(mapd) else "MAPD_like_nonzero\tNA\n")
            s.write(f"GC_Spearman_nonzero\t{gc_spear:.6f}\n" if not math.isnan(gc_spear) else "GC_Spearman_nonzero\tNA\n")
            s.write(f"GC_Pearson_nonzero\t{gc_pear:.6f}\n" if not math.isnan(gc_pear) else "GC_Pearson_nonzero\tNA\n")

        row = [
            sample, bin_label, bins_total,
            f"{gc_valid_frac:.6f}", f"{dropout_frac:.6f}",
            f"{mean_all:.6f}", f"{med_all:.6f}",
            f"{mean_nz:.6f}" if not math.isnan(mean_nz) else "NA",
            f"{med_nz:.6f}" if not math.isnan(med_nz) else "NA",
            f"{iqr_nz:.6f}" if not math.isnan(iqr_nz) else "NA",
            f"{mad_nz:.6f}" if not math.isnan(mad_nz) else "NA",
            f"{cv_nz:.6f}" if not math.isnan(cv_nz) else "NA",
            f"{gini_nz:.6f}" if not math.isnan(gini_nz) else "NA",
            f"{top1:.6f}" if not math.isnan(top1) else "NA",
            f"{top5:.6f}" if not math.isnan(top5) else "NA",
            f"{top10:.6f}" if not math.isnan(top10) else "NA",
            f"{mapd:.6f}" if not math.isnan(mapd) else "NA",
            f"{gc_spear:.6f}" if not math.isnan(gc_spear) else "NA",
            f"{gc_pear:.6f}" if not math.isnan(gc_pear) else "NA",
        ] + [f"{breadths[t]:.6f}" for t in THRESHOLDS]

        master_rows.append(row)

    print(f"Writing master CSV: {master_csv}")
    with master_csv.open("w", newline="") as f:
        w = csv.writer(f)
        w.writerow(master_header)
        w.writerows(master_rows)

print("DONE (mosdepth bin QC extraction: 10kb + 50kb)")
PY

# ============================================================
# STEP 4: Extract Picard metrics into a master CSV (expanded)
# Parses:
#   - picard.MarkDuplicates.metrics.txt
#   - picard.gc_bias_summary.txt
#   - picard.wgs_metrics.txt
# Output:
#   - $MAP_ROOT/master_picard_metrics.csv
# ============================================================

export MAP_ROOT

python3 - <<'PY'
import os, csv, re
from pathlib import Path

MAP_ROOT = Path(os.environ["MAP_ROOT"])

def parse_picard_first_table(path: Path):
    """
    Generic Picard .txt metrics parser:
      - Skips comment lines starting with '#'
      - Finds a header row (>=2 columns)
      - Reads the next non-empty, non-comment row as values
    Returns {col: val} for the first data row found.
    """
    if not path.exists():
        return {}
    lines = [ln.rstrip("\n") for ln in path.open("rt", errors="replace")]
    i = 0
    while i < len(lines):
        line = lines[i].strip()
        if not line or line.startswith("#"):
            i += 1
            continue

        # try tab-delimited first
        header = re.split(r"\t+", line)
        if len(header) < 2:
            header = re.split(r"\s+", line)
        if len(header) < 2:
            i += 1
            continue

        # next data row
        j = i + 1
        while j < len(lines) and (not lines[j].strip() or lines[j].lstrip().startswith("#")):
            j += 1
        if j >= len(lines):
            return {}

        values = re.split(r"\t+", lines[j].strip())
        if len(values) != len(header):
            values = re.split(r"\s+", lines[j].strip())

        if len(values) != len(header):
            i += 1
            continue

        return dict(zip(header, values))
    return {}

def pick(d, key):
    return d.get(key, "")

samples = sorted([p for p in MAP_ROOT.iterdir() if p.is_dir()])

out_csv = MAP_ROOT / "master_picard_metrics.csv"

# Expanded set of useful Picard fields (only filled if present in files)
header = [
    "Sample",

    # MarkDuplicates
    "READ_PAIRS_EXAMINED",
    "UNPAIRED_READS_EXAMINED",
    "UNMAPPED_READS",
    "READ_PAIR_DUPLICATES",
    "UNPAIRED_READ_DUPLICATES",
    "PERCENT_DUPLICATION",
    "ESTIMATED_LIBRARY_SIZE",

    # GC bias summary (CollectGcBiasMetrics)
    "WINDOW_SIZE",
    "TOTAL_CLUSTERS",
    "AT_DROPOUT",
    "GC_DROPOUT",
    "MEAN_BIAS",

    # WGS metrics (CollectWgsMetrics)
    "GENOME_TERRITORY",
    "MEAN_COVERAGE",
    "SD_COVERAGE",
    "MEDIAN_COVERAGE",
    "MAD_COVERAGE",

    "PCT_EXC_ADAPTER",
    "PCT_EXC_MAPQ",
    "PCT_EXC_DUPE",
    "PCT_EXC_BASEQ",
    "PCT_EXC_OVERLAP",
    "PCT_EXC_CAPPED",
    "PCT_EXC_TOTAL",

    "PCT_0X",
    "PCT_1X",
    "PCT_5X",
    "PCT_10X",
    "PCT_20X",
    "PCT_30X",
    "PCT_50X",
    "PCT_100X",
]

rows = []

for sd in samples:
    sample = sd.name

    md = parse_picard_first_table(sd / "picard.MarkDuplicates.metrics.txt")
    gc = parse_picard_first_table(sd / "picard.gc_bias_summary.txt")
    wgs = parse_picard_first_table(sd / "picard.wgs_metrics.txt")

    row = {
        "Sample": sample,

        "READ_PAIRS_EXAMINED": pick(md, "READ_PAIRS_EXAMINED"),
        "UNPAIRED_READS_EXAMINED": pick(md, "UNPAIRED_READS_EXAMINED"),
        "UNMAPPED_READS": pick(md, "UNMAPPED_READS"),
        "READ_PAIR_DUPLICATES": pick(md, "READ_PAIR_DUPLICATES"),
        "UNPAIRED_READ_DUPLICATES": pick(md, "UNPAIRED_READ_DUPLICATES"),
        "PERCENT_DUPLICATION": pick(md, "PERCENT_DUPLICATION"),
        "ESTIMATED_LIBRARY_SIZE": pick(md, "ESTIMATED_LIBRARY_SIZE"),

        "WINDOW_SIZE": pick(gc, "WINDOW_SIZE"),
        "TOTAL_CLUSTERS": pick(gc, "TOTAL_CLUSTERS"),
        "AT_DROPOUT": pick(gc, "AT_DROPOUT"),
        "GC_DROPOUT": pick(gc, "GC_DROPOUT"),
        "MEAN_BIAS": pick(gc, "MEAN_BIAS") or pick(gc, "MEAN_BIAS_COVERAGE"),

        "GENOME_TERRITORY": pick(wgs, "GENOME_TERRITORY"),
        "MEAN_COVERAGE": pick(wgs, "MEAN_COVERAGE"),
        "SD_COVERAGE": pick(wgs, "SD_COVERAGE"),
        "MEDIAN_COVERAGE": pick(wgs, "MEDIAN_COVERAGE"),
        "MAD_COVERAGE": pick(wgs, "MAD_COVERAGE"),

        "PCT_EXC_ADAPTER": pick(wgs, "PCT_EXC_ADAPTER"),
        "PCT_EXC_MAPQ": pick(wgs, "PCT_EXC_MAPQ"),
        "PCT_EXC_DUPE": pick(wgs, "PCT_EXC_DUPE"),
        "PCT_EXC_BASEQ": pick(wgs, "PCT_EXC_BASEQ"),
        "PCT_EXC_OVERLAP": pick(wgs, "PCT_EXC_OVERLAP"),
        "PCT_EXC_CAPPED": pick(wgs, "PCT_EXC_CAPPED"),
        "PCT_EXC_TOTAL": pick(wgs, "PCT_EXC_TOTAL"),

        "PCT_0X": pick(wgs, "PCT_0X"),
        "PCT_1X": pick(wgs, "PCT_1X"),
        "PCT_5X": pick(wgs, "PCT_5X"),
        "PCT_10X": pick(wgs, "PCT_10X"),
        "PCT_20X": pick(wgs, "PCT_20X"),
        "PCT_30X": pick(wgs, "PCT_30X"),
        "PCT_50X": pick(wgs, "PCT_50X"),
        "PCT_100X": pick(wgs, "PCT_100X"),
    }

    rows.append([row.get(h, "") for h in header])

with out_csv.open("w", newline="") as f:
    w = csv.writer(f)
    w.writerow(header)
    w.writerows(rows)

print(f"Wrote: {out_csv}")
PY

echo "ALL STEPS COMPLETE."
echo "Refmap master CSV:    $MASTER_REFMAP_CSV"
echo "Mosdepth bin masters: $MAP_ROOT/master_bin_qc_metrics_10kb.csv and $MAP_ROOT/master_bin_qc_metrics_50kb.csv"
echo "Picard master CSV:    $MAP_ROOT/master_picard_metrics.csv"
