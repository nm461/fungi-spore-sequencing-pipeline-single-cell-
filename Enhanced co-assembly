#!/bin/bash
#$ -S /bin/bash
#$ -N coassembly_sequential
#$ -cwd
#$ -o coassembly_sequential.out
#$ -e coassembly_sequential.err
#$ -l h_vmem=64G
#$ -pe def_slot 8
#$ -l h_rt=48:00:00

set -euo pipefail
set -x

# =========================
# Modules / Environment
# =========================
module use /usr/local/package/modulefiles
module load apptainer
module load blast+/2.15.0
module load repeatmasker/4.1.6 || true

export PATH=~/coassembly/bin/SPAdes-4.2.0-Linux/bin:$PATH
export PATH="$HOME/tools/rmblast/rmblast-2.14.1/bin:$PATH"

THREADS="${NSLOTS:-8}"

# =========================
# Paths (adjust if needed)
# =========================

# IMPORTANT: This must be the same directory where your master pipeline wrote master_report/aggregated_metrics.csv
OUT_ROOT="$HOME/Enhanced_Pipeline_10_3_out"

# Singularity images
SIF_DIR="$HOME/sif_images"

# Kraken2 database
DB_DIR="$HOME/databases/kraken2_plusPFP_16G/k2_pluspfp_16G"

# BUSCO lineage dataset
BUSCO_LINEAGE="$HOME/fungi_odb10"

# Sourmash database
SM_DB="$HOME/databases/sourmash/fungi/genbank-2022.03-fungi-k21.zip"


COA_DIR="$OUT_ROOT/co-assembly"
TOP_DIR="$COA_DIR/top_samples"
SEQ_DIR="$COA_DIR/sequential_assemblies"
MASTER_DIR="$OUT_ROOT/master_report"
METRICS_CSV="$MASTER_DIR/aggregated_metrics.csv"

mkdir -p "$COA_DIR" "$TOP_DIR" "$SEQ_DIR"/{double,triple,quadruple,pentuple,sextuple} "$COA_DIR/tools"

export OUT_ROOT SIF_DIR DB_DIR BUSCO_LINEAGE SM_DB

# =========================
# Python virtual environment
# =========================
SHARED_VENV="$OUT_ROOT/shared_venv"
if [ ! -d "$SHARED_VENV" ]; then
  python3 -m venv "$SHARED_VENV"
  source "$SHARED_VENV/bin/activate"
  pip install --upgrade pip
  pip install --no-cache-dir pandas numpy matplotlib seaborn biopython scipy PyPDF2
  deactivate
fi

# =========================
# Step 1: Select Top 6 Samples by BUSCO completeness
# =========================
source "$SHARED_VENV/bin/activate"
python3 - << 'PY_SELECT'
import os, pandas as pd, shutil, sys

# Also need to change this outroot depending on your asssembly run
OUT_ROOT = os.environ.get("OUT_ROOT", os.path.expanduser("~/Enhanced_Pipeline_10_3out"))                                        
COA_DIR  = os.path.join(OUT_ROOT, "co-assembly")
TOP_DIR  = os.path.join(COA_DIR, "top_samples")
MASTER   = os.path.join(OUT_ROOT, "master_report")
CSV      = os.path.join(MASTER, "aggregated_metrics.csv")

if not os.path.exists(CSV):
    sys.exit(f"ERROR: BUSCO metrics not found: {CSV}")

df = pd.read_csv(CSV, index_col=0)
if "Complete" not in df.columns:
    sys.exit("ERROR: 'Complete' column missing in aggregated_metrics.csv")

top6 = df["Complete"].dropna().sort_values(ascending=False).head(6)
if top6.empty:
    sys.exit("ERROR: No samples with BUSCO completeness found.")

# save ranked list
os.makedirs(COA_DIR, exist_ok=True)
rank_txt = os.path.join(COA_DIR, "top6_samples.txt")
rank_csv = os.path.join(COA_DIR, "top6_ranked_samples.csv")
top6.index.to_series().to_csv(rank_txt, index=False, header=False)
top6.to_csv(rank_csv)
print("Top-6 samples:\n", top6)

# copy trimmed reads into co-assembly/top_samples/<sample>/
os.makedirs(TOP_DIR, exist_ok=True)
for sample in top6.index:
    sdir = os.path.join(OUT_ROOT, f"{sample}_out")
    if not os.path.isdir(sdir):
        print(f"WARNING: {sdir} not found; skipping")
        continue
    dest = os.path.join(TOP_DIR, sample)
    os.makedirs(dest, exist_ok=True)
    found = 0
    for suffix in ["_R1.fastq.gz", "_R2.fastq.gz"]:
        for cand in [
            os.path.join(sdir, "raw_outputs", "fastp", f"trimmed{suffix}"),
            os.path.join(sdir, f"trimmed{suffix}")
        ]:
            if os.path.exists(cand):
                shutil.copy2(cand, os.path.join(dest, os.path.basename(cand)))
                print(f"Copied {os.path.basename(cand)} -> {dest}")
                found += 1
                break
    if found < 2:
        print(f"WARNING: missing one or more trimmed reads for {sample}")
PY_SELECT
deactivate

# =========================
# Step 2: Per-assembly report generator (full master-style)
# =========================
cat > "$COA_DIR/tools/make_sample_report.py" << 'PY_PER_SAMPLE'
import os, json, gzip, re, glob, subprocess, shutil, datetime
import numpy as np, pandas as pd, matplotlib.pyplot as plt, seaborn as sns
from matplotlib.backends.backend_pdf import PdfPages
from matplotlib import font_manager as fm
from matplotlib.ticker import MaxNLocator
from Bio import SeqIO
from PyPDF2 import PdfMerger

OUT_DIR=os.environ.get('OUT_DIR','')
SIF_DIR=os.path.expanduser(os.environ.get('SIF_DIR','~/sif_images'))
if not OUT_DIR:
    raise SystemExit("OUT_DIR not set")

sns.set_style("whitegrid")
plt.rcParams['figure.dpi']=120

# --------- Choose a JP-capable font if available (for summary page) ---------
def pick_jp_font():
    preferred = ['Noto Sans CJK JP', 'NotoSansCJK-Regular', 'IPAGothic', 'IPAexGothic',
                 'TakaoGothic', 'VL Gothic', 'Source Han Sans JP', 'SourceHanSansJP']
    paths = fm.findSystemFonts(fontpaths=None, fontext='ttf') + fm.findSystemFonts(fontpaths=None, fontext='ttc')
    names = {}
    for p in paths:
        try:
            prop = fm.FontProperties(fname=p)
            names[fm.get_font(p).family_name] = p
        except Exception:
            pass
    for name in preferred:
        if name in names:
            return fm.FontProperties(fname=names[name])
    return None

JP_FONT = pick_jp_font()

def annot(ax, text):
    ax.text(0.98,0.95,text,transform=ax.transAxes,ha="right",va="top",
            fontsize=8,bbox=dict(facecolor='white',alpha=0.65,edgecolor='none'))

def ensure_depth():
    depth = os.path.join(OUT_DIR, "depth.txt")
    bam = os.path.join(OUT_DIR, "mapped.sorted.bam")
    sif = os.path.join(SIF_DIR, "samtools.sif")
    if os.path.exists(depth): return depth
    if os.path.exists(bam) and os.path.exists(sif):
        with open(depth,"w") as out:
            subprocess.call(["singularity","exec",sif,"samtools","depth","-a",bam], stdout=out)
        return depth
    return None

# Parse fastp JSON (preferred) or derive from trimmed FASTQs
read_qual = {}
fjson=os.path.join(OUT_DIR,"fastp.json")
if os.path.exists(fjson):
    try:
        j=json.load(open(fjson))
        r1=j.get("read1_after_filtering",{}).get("quality_curves",{}).get("mean")
        r2=j.get("read2_after_filtering",{}).get("quality_curves",{}).get("mean")
        if not r1:
            r1=j.get("read1_before_filtering",{}).get("quality_curves",{}).get("mean")
        if not r2:
            r2=j.get("read2_before_filtering",{}).get("quality_curves",{}).get("mean")
        pbq=j.get("per_base_quality",{}).get("mean")
        if r1: read_qual["R1"]=r1
        if r2: read_qual["R2"]=r2
        if (not r1 or not r2) and pbq:
            read_qual["fastp"]=pbq
    except: pass

def sample_fastq_means(path, max_reads=20000):
    if not os.path.exists(path): return None
    means=[]; cnts=[]
    with gzip.open(path,"rt") as fh:
        for i,rec in enumerate(SeqIO.parse(fh,"fastq"),1):
            quals=rec.letter_annotations.get("phred_quality",[])
            for k,q in enumerate(quals):
                if k>=len(means):
                    means.extend([0]*(k+1-len(means)))
                    cnts.extend([0]*(k+1-len(cnts)))
                means[k]+=q; cnts[k]+=1
            if i>=max_reads: break
    if not cnts: return None
    return [means[i]/cnts[i] for i in range(len(means))]

r1p=os.path.join(OUT_DIR,"trimmed_R1.fastq.gz")
r2p=os.path.join(OUT_DIR,"trimmed_R2.fastq.gz")
if "R1" not in read_qual:
    m1=sample_fastq_means(r1p); 
    if m1: read_qual["R1"]=m1
if "R2" not in read_qual:
    m2=sample_fastq_means(r2p)
    if m2: read_qual["R2"]=m2

# Contigs & stats
fa=os.path.join(OUT_DIR,"spades_out","contigs.fasta")
contig_lengths=[]; n50=0; total=0; max_ctg=0
if os.path.exists(fa):
    contigs=list(SeqIO.parse(fa,"fasta"))
    contig_lengths=[len(c.seq) for c in contigs]
    if contig_lengths:
        total=sum(contig_lengths); max_ctg=max(contig_lengths)
        s=sorted(contig_lengths,reverse=True)
        csum=0
        for L in s:
            csum+=L
            if csum>=total/2:
                n50=L; break

# BUSCO: latest log (support both master-style and co-assembly naming/paths)
busco_vals={}; busco_n=None
cands=[]
cands += glob.glob(os.path.join(OUT_DIR,"*.log"))
cands += glob.glob(os.path.join(OUT_DIR,"busco_*.log"))
cands += glob.glob(os.path.join(OUT_DIR,"logs","busco_*.log"))
cands = sorted(cands, key=os.path.getmtime, reverse=True)
for blog in cands:
    for line in open(blog, encoding='utf-8', errors='ignore'):
        m=re.search(r"C:(\d+\.\d+)%.*D:(\d+\.\d+)%.*F:(\d+\.\d+)%.*M:(\d+\.\d+)%.*n:(\d+)", line)
        if m:
            vals=list(m.groups())
            busco_vals=dict(zip(["Complete","Duplicated","Fragmented","Missing"], map(float, vals[:4])))
            busco_n=int(vals[4]); break
    if busco_vals: break

# Kraken top taxa
krep=os.path.join(OUT_DIR,"kraken_report.txt")
kdf=None
if os.path.exists(krep):
    try:
        kdf=pd.read_csv(krep,sep="\t",header=None,
                        names=["perc","reads","tax_reads","rank","taxid","name"])
        kdf["name"]=kdf["name"].astype(str).str.strip()
    except: pass

# Depth
depth_path=ensure_depth()
ddf=None
if depth_path and os.path.exists(depth_path):
    try:
        ddf=pd.read_csv(depth_path,sep="\t",header=None,names=["contig","pos","depth"])
    except: pass

# --------- MULTI-PAGE PDF (graphs) ---------
mp_pdf=os.path.join(OUT_DIR,"summary_plots.pdf")
with PdfPages(mp_pdf) as pdf:
    # Read quality
    if read_qual:
        fig,ax=plt.subplots(figsize=(6,4))
        if "R1" in read_qual: ax.plot(read_qual["R1"], label="R1", color="blue")
        if "R2" in read_qual: ax.plot(read_qual["R2"], label="R2", color="orange")
        if "fastp" in read_qual: ax.plot(read_qual["fastp"], label="fastp(mean)", color="green", linestyle="--")
        ax.set_title("Per-base read quality")
        ax.set_xlabel("Base position")
        ax.set_ylabel("Mean Phred quality")
        ax.legend(fontsize=8); ax.grid(True,alpha=0.3)
        allq=[]
        for k in read_qual: allq.extend(read_qual[k][:150])
        if allq:
            annot(ax, f"Mean={np.mean(allq):.1f}  SD={np.std(allq):.1f}")
        fig.tight_layout(); pdf.savefig(fig); plt.close(fig)

    # Contig length histogram in Mb
    if contig_lengths:
        x = np.array(contig_lengths)/1e6
        fig,ax=plt.subplots(figsize=(6,4))
        ax.hist(x, bins=50, color="purple", edgecolor="black", alpha=0.8, log=True)
        ax.set_title("Contig length distribution")
        ax.set_xlabel("Contig length (Mb)")
        ax.set_ylabel("Count (log scale)")
        ax.xaxis.set_major_locator(MaxNLocator(nbins=8))
        annot(ax, f"N50={n50/1e6:.3f} Mb  Total={total/1e6:.2f} Mb  n={len(contig_lengths)}")
        fig.tight_layout(); pdf.savefig(fig); plt.close(fig)

    # BUSCO per-sample (bar)
    if busco_vals:
        fig,ax=plt.subplots(figsize=(6,4))
        order=["Complete","Duplicated","Fragmented","Missing"]
        cols=["forestgreen","royalblue","darkorange","red"]
        ax.bar(order,[busco_vals.get(k,0) for k in order], color=cols, alpha=0.9)
        ax.set_ylim(0,100)
        ax.set_ylabel("Percentage (%)")
        ax.set_title("BUSCO completeness (per-assembly)")
        ax.tick_params(axis='x', labelrotation=0)
        annot(ax, f"C={busco_vals.get('Complete',np.nan):.1f}%  D={busco_vals.get('Duplicated',np.nan):.1f}%  F={busco_vals.get('Fragmented',np.nan):.1f}%")
        fig.tight_layout(); pdf.savefig(fig); plt.close(fig)

    # Kraken top taxa
    if kdf is not None and not kdf.empty:
        top = kdf.sort_values("perc",ascending=False).head(10)
        fig,ax=plt.subplots(figsize=(6,4))
        sns.barplot(y=top["name"], x=top["perc"], palette="crest", ax=ax)
        ax.set_title("Kraken2: Top taxa")
        ax.set_xlabel("% of reads (Kraken2)")
        ax.set_ylabel("Taxon")
        fig.tight_layout(); pdf.savefig(fig); plt.close(fig)

    # Coverage depth histogram
    if ddf is not None and not ddf.empty:
        fig,ax=plt.subplots(figsize=(6,4))
        ax.hist(ddf["depth"], bins=100, color="seagreen", edgecolor="black", alpha=0.85, log=True)
        ax.set_title("Coverage depth histogram")
        ax.set_xlabel("Coverage depth (×)")
        ax.set_ylabel("Count (log scale)")
        annot(ax, f"Mean={ddf['depth'].mean():.1f}×  Median={ddf['depth'].median():.1f}×")
        fig.tight_layout(); pdf.savefig(fig); plt.close(fig)

# --------- SINGLE-PAGE PANEL ---------
panel_pdf=os.path.join(OUT_DIR,"summary_panel.pdf")
fig, axes = plt.subplots(2,3, figsize=(12,8))
axes = axes.flatten()

# 1: Read quality
ax=axes[0]
if read_qual:
    if "R1" in read_qual: ax.plot(read_qual["R1"], label="R1", color="blue")
    if "R2" in read_qual: ax.plot(read_qual["R2"], label="R2", color="orange")
    if "fastp" in read_qual: ax.plot(read_qual["fastp"], label="fastp", color="green", linestyle="--")
    ax.set_title("Read quality")
    ax.set_xlabel("Base position")
    ax.set_ylabel("Mean Phred quality")
    ax.legend(fontsize=7)
else:
    ax.text(0.5,0.5,"No read quality",ha="center",va="center"); ax.set_axis_off()

# 2: Contig lengths
ax=axes[1]
if contig_lengths:
    ax.hist(np.array(contig_lengths)/1e6, bins=40, color="purple", edgecolor="black", alpha=0.85, log=True)
    ax.set_title("Contig lengths (log count)")
    ax.set_xlabel("Mb")
    ax.set_ylabel("Count (log)")
else:
    ax.text(0.5,0.5,"No contigs",ha="center",va="center"); ax.set_axis_off()

# 3: BUSCO
ax=axes[2]
if busco_vals:
    order=["Complete","Duplicated","Fragmented","Missing"]
    cols=["forestgreen","royalblue","darkorange","red"]
    ax.bar(order,[busco_vals.get(k,0) for k in order], color=cols)
    ax.set_ylim(0,100)
    ax.set_title("BUSCO")
    ax.set_ylabel("Percentage (%)")
else:
    ax.text(0.5,0.5,"No BUSCO",ha="center",va="center"); ax.set_axis_off()

# 4: Kraken
ax=axes[3]
if kdf is not None and not kdf.empty:
    top=kdf.sort_values("perc",ascending=False).head(10)
    ax.barh(top["name"], top["perc"], color="teal", alpha=0.9)
    ax.set_xlabel("% of reads (Kraken2)")
    ax.set_ylabel("Taxon")
    ax.invert_yaxis(); ax.set_title("Kraken top taxa")
else:
    ax.text(0.5,0.5,"No Kraken",ha="center",va="center"); ax.set_axis_off()

# 5: Coverage
ax=axes[4]
if ddf is not None and not ddf.empty:
    ax.hist(ddf["depth"], bins=80, color="seagreen", edgecolor="black", alpha=0.85, log=True)
    ax.set_title("Coverage depth")
    ax.set_xlabel("Coverage depth (×)")
    ax.set_ylabel("Count (log)")
else:
    ax.text(0.5,0.5,"No depth",ha="center",va="center"); ax.set_axis_off()

# 6: Assembly stats text
ax=axes[5]; ax.axis('off')
txt = []
if total>0: txt.append(f"Total: {total/1e6:.2f} Mb")
if n50>0:   txt.append(f"N50: {n50/1e6:.3f} Mb")
if max_ctg>0: txt.append(f"Max contig: {max_ctg/1e6:.3f} Mb")
if contig_lengths: txt.append(f"Contigs: {len(contig_lengths)}")
if busco_vals: txt.append(f"BUSCO C={busco_vals.get('Complete',np.nan):.1f}%")
ax.text(0.02,0.6,"\n".join(txt) if txt else "No assembly stats", fontsize=10)

fig.tight_layout()
fig.savefig(panel_pdf, dpi=300)
plt.close(fig)

# -------- Japanese summary.txt --------
q30=None; r1len=None; r2len=None; pairs_before=None; pairs_after=None
if os.path.exists(fjson):
    try:
        jj=json.load(open(fjson))
        q30 = jj.get("summary",{}).get("after_filtering",{}).get("q30_rate")
        r1len = jj.get("summary",{}).get("after_filtering",{}).get("read1_mean_length")
        r2len = jj.get("summary",{}).get("after_filtering",{}).get("read2_mean_length")
        pairs_before = jj.get("summary",{}).get("before_filtering",{}).get("total_reads")
        pairs_after  = jj.get("summary",{}).get("after_filtering",{}).get("total_reads")
    except: pass

kraken_top_text=[]
if kdf is not None and not kdf.empty:
    top = kdf.sort_values("perc", ascending=False).head(5)
    for _,r in top.iterrows():
        kraken_top_text.append(f"{r['name']} ...... {r['perc']:.1f}%")

summary_path=os.path.join(OUT_DIR,"summary.txt")
with open(summary_path,"w",encoding="utf-8") as w:
    w.write("=== サンプル概要 / CO-ASSEMBLY シーケンシャル ===\n")
    w.write(f"実行日時          : {datetime.datetime.now().strftime('%Y-%m-%d %H:%M %Z')}\n")
    w.write(f"出力ディレクトリ  : {OUT_DIR}\n")
    w.write("処理フローと役割：\n")
    w.write("  SPAdes (--sc) : 単一細胞モードのco-assembly\n")
    w.write("  Kraken2       : タクソノミー分類（リードの出自推定）\n")
    w.write("  BUSCO         : 系統特異的遺伝子セットに対する網羅率評価\n")
    w.write("  BWA/Samtools  : マッピング・深さ集計\n")
    w.write("  Pilon         : マッピングに基づくアセンブリのポリッシング\n")
    w.write("  Sourmash      : k-mer スケッチと類似性評価\n\n")
    w.write("------------------------------------------------------------\n")
    w.write("[1] Fastp 品質管理（参考）\n")
    if pairs_before is not None and pairs_after is not None:
        drop = 100.0*(pairs_before-pairs_after)/max(1.0,pairs_before)
        w.write(f"  リード数（前 → 後）: {int(pairs_before):,} → {int(pairs_after):,}（-{drop:.1f}%）\n")
    if q30 is not None: w.write(f"  Q30塩基割合              : {100.0*float(q30):.1f}%\n")
    if r1len is not None and r2len is not None:
        w.write(f"  平均リード長（R1/R2）    : {float(r1len):.1f} / {float(r2len):.1f} bp\n")
    w.write("\n")
    w.write("[2] SPAdes アセンブリ結果\n")
    if contig_lengths:
        w.write(f"  コンティグ数             : {len(contig_lengths):,}\n")
        w.write(f"  総長                      : {total/1e6:.2f} Mb\n")
        w.write(f"  N50                      : {n50/1e6:.3f} Mb\n")
        w.write(f"  最大コンティグ            : {max_ctg/1e6:.3f} Mb\n")
    w.write("  出力FASTA                : raw_outputs/spades/contigs.fasta\n\n")
    w.write("[3] Kraken2 タクソノミー（上位）\n")
    if kraken_top_text:
        for t in kraken_top_text: w.write(f"  {t}\n")
    w.write("  レポート               : raw_outputs/kraken2/kraken_report.txt\n\n")
    w.write("[4] BUSCO 網羅率\n")
    if busco_vals:
        w.write(f"  Complete（完全）        : {busco_vals.get('Complete',0):.1f}%\n")
        w.write(f"    └ Duplicated（重複）  : {busco_vals.get('Duplicated',0):.1f}%\n")
        w.write(f"  Fragmented（断片）      : {busco_vals.get('Fragmented',0):.1f}%\n")
        w.write(f"  Missing（欠損）         : {busco_vals.get('Missing',0):.1f}%\n")
        if busco_n: w.write(f"  データセット n         : {busco_n}\n")
    w.write("  実行ログ               : logs/busco_*.log\n\n")
    w.write("[5] マッピングとカバレッジ（BWA/Samtools）\n")
    w.write("  BAM                     : raw_outputs/mapping/mapped.sorted.bam（+ .bai）\n")
    if ddf is not None and not ddf.empty:
        w.write(f"  平均被覆深さ（mean / median）: {ddf['depth'].mean():.1f}× / {ddf['depth'].median():.1f}×\n")
    w.write("  深さ表（samtools depth） : raw_outputs/mapping/depth.txt\n\n")
    w.write("[6] Pilon ポリッシング\n")
    w.write("  出力FASTA               : raw_outputs/pilon/pilon_corrected.fasta\n")
    w.write("  ログ                    : logs/pilon.log\n\n")
    w.write("[7] Sourmash 類似性（gather）\n")
    w.write("  出力（sig/CSV）         : raw_outputs/sourmash/assembly.sig, gather_results.csv\n\n")
    w.write("------------------------------------------------------------\n")
    w.write("図（graphs/）とログ（logs/）に詳細があります。\n")

# --------- Save main figure PNGs for graphs/ ---------
def save_single_png(fig_func, path):
    fig = fig_func()
    if fig is not None:
        fig.savefig(path, dpi=300)
        plt.close(fig)

def fig_readq():
    if not read_qual: return None
    fig,ax=plt.subplots(figsize=(6,4))
    if "R1" in read_qual: ax.plot(read_qual["R1"], label="R1", color="blue")
    if "R2" in read_qual: ax.plot(read_qual["R2"], label="R2", color="orange")
    if "fastp" in read_qual: ax.plot(read_qual["fastp"], label="fastp", color="green", linestyle="--")
    ax.set_title("Per-base read quality"); ax.set_xlabel("Base position"); ax.set_ylabel("Mean Phred quality")
    ax.legend(fontsize=8); ax.grid(True,alpha=0.3); fig.tight_layout(); return fig

def fig_contigs():
    if not contig_lengths: return None
    fig,ax=plt.subplots(figsize=(6,4))
    ax.hist(np.array(contig_lengths)/1e6, bins=50, color="purple", edgecolor="black", alpha=0.8, log=True)
    ax.set_title("Contig length distribution"); ax.set_xlabel("Contig length (Mb)"); ax.set_ylabel("Count (log scale)")
    fig.tight_layout(); return fig

def fig_busco():
    if not busco_vals: return None
    fig,ax=plt.subplots(figsize=(6,4))
    order=["Complete","Duplicated","Fragmented","Missing"]; cols=["forestgreen","royalblue","darkorange","red"]
    ax.bar(order,[busco_vals.get(k,0) for k in order], color=cols, alpha=0.9)
    ax.set_ylim(0,100); ax.set_ylabel("Percentage (%)"); ax.set_title("BUSCO completeness (per-assembly)")
    fig.tight_layout(); return fig

def fig_kraken():
    if kdf is None or kdf.empty: return None
    top = kdf.sort_values("perc",ascending=False).head(10)
    fig,ax=plt.subplots(figsize=(6,4))
    sns.barplot(y=top["name"], x=top["perc"], palette="crest", ax=ax)
    ax.set_title("Kraken2: Top taxa"); ax.set_xlabel("% of reads (Kraken2)"); ax.set_ylabel("Taxon")
    fig.tight_layout(); return fig

def fig_cov():
    if ddf is None or ddf.empty: return None
    fig,ax=plt.subplots(figsize=(6,4))
    ax.hist(ddf["depth"], bins=100, color="seagreen", edgecolor="black", alpha=0.85, log=True)
    ax.set_title("Coverage depth histogram"); ax.set_xlabel("Coverage depth (×)"); ax.set_ylabel("Count (log scale)")
    fig.tight_layout(); return fig

save_single_png(fig_readq, os.path.join(OUT_DIR,"fig_read_quality.png"))
save_single_png(fig_contigs, os.path.join(OUT_DIR,"fig_contig_lengths.png"))
save_single_png(fig_busco, os.path.join(OUT_DIR,"fig_busco.png"))
save_single_png(fig_kraken, os.path.join(OUT_DIR,"fig_kraken_top.png"))
save_single_png(fig_cov, os.path.join(OUT_DIR,"fig_coverage_hist.png"))

# --------- Build summary PAGE 1 as PDF (Japanese font if available) ---------
summary_pdf=os.path.join(OUT_DIR,"summary_page.pdf")
txt=open(summary_path, "r", encoding="utf-8", errors="ignore").read()
fig,ax=plt.subplots(figsize=(8.5,11))
ax.axis('off')
if JP_FONT:
    ax.text(0.06, 0.96, txt, va='top', ha='left', fontproperties=JP_FONT, fontsize=11, linespacing=1.25)
else:
    ax.text(0.06, 0.96, txt, va='top', ha='left', family='monospace', fontsize=10, linespacing=1.2)
fig.tight_layout()
fig.savefig(summary_pdf, dpi=300)
plt.close(fig)

# --------- Combined per-sample PDF: summary_page (p1) + panel (p2) ---------
combined_pdf=os.path.join(OUT_DIR,"sample_report.pdf")
merger=PdfMerger()
if os.path.exists(summary_pdf): merger.append(summary_pdf)
if os.path.exists(panel_pdf):   merger.append(panel_pdf)
merger.write(combined_pdf); merger.close()
print("Per-assembly combined PDF:", combined_pdf)
PY_PER_SAMPLE


# =========================
# Step 3: Sequential co-assembly function
# =========================
run_seq_coassembly() {
  local K="$1" LABEL="$2"
  local OUT_DIR="$SEQ_DIR/$LABEL"
  mkdir -p "$OUT_DIR"

  # Build sample list (top K)
  mapfile -t SAMPLES < <(head -n "$K" "$COA_DIR/top6_samples.txt")

  # Collect reads
  local R1_LIST=() R2_LIST=()
  for s in "${SAMPLES[@]}"; do
    local R1="$TOP_DIR/$s/trimmed_R1.fastq.gz"
    local R2="$TOP_DIR/$s/trimmed_R2.fastq.gz"
    [ -f "$R1" ] || { echo "Missing $R1"; exit 1; }
    [ -f "$R2" ] || { echo "Missing $R2"; exit 1; }
    R1_LIST+=("$R1"); R2_LIST+=("$R2")
  done

  # Combine reads
  zcat "${R1_LIST[@]}" | gzip > "$OUT_DIR/R1_combined.fastq.gz"
  zcat "${R2_LIST[@]}" | gzip > "$OUT_DIR/R2_combined.fastq.gz"

  # ---------- SPAdes ----------
  spades.py --sc \
    -1 "$OUT_DIR/R1_combined.fastq.gz" \
    -2 "$OUT_DIR/R2_combined.fastq.gz" \
    -o "$OUT_DIR/spades_out" \
    --threads "$THREADS" --memory 64 \
    > "$OUT_DIR/spades.log" 2>&1
  local ASSEMBLY="$OUT_DIR/spades_out/contigs.fasta"

  # ---------- Kraken2 ----------
  singularity exec "$SIF_DIR/kraken2.sif" kraken2 \
    --db "$DB_DIR" \
    --report "$OUT_DIR/kraken_report.txt" \
    --paired "$OUT_DIR/R1_combined.fastq.gz" "$OUT_DIR/R2_combined.fastq.gz" \
    --threads "$THREADS" \
    > "$OUT_DIR/kraken_output.txt" 2>&1

  # ---------- BUSCO ----------
  export OMP_NUM_THREADS=1 OPENBLAS_NUM_THREADS=1 NUMEXPR_NUM_THREADS=1 MKL_NUM_THREADS=1
  local TS; TS=$(date +%Y%m%d%H%M%S)
  local BUSCO_RUN="busco_${LABEL}_${TS}"
  local BUSCO_LOG="$OUT_DIR/${BUSCO_RUN}.log"
  singularity exec -B "$OUT_DIR:$OUT_DIR" "$SIF_DIR/busco_v6.0.0_cv1.sif" \
    busco -i "$ASSEMBLY" -o "$BUSCO_RUN" -l "$BUSCO_LINEAGE" -m genome --cpu 2 --out_path "$OUT_DIR" \
    > "$BUSCO_LOG" 2>&1

  # ---------- BWA/Samtools ----------
  singularity exec "$SIF_DIR/bwa.sif" bwa index "$ASSEMBLY"
  singularity exec "$SIF_DIR/bwa.sif" bwa mem -t "$THREADS" \
    "$ASSEMBLY" "$OUT_DIR/R1_combined.fastq.gz" "$OUT_DIR/R2_combined.fastq.gz" \
    | singularity exec "$SIF_DIR/samtools.sif" samtools view -bS - \
    > "$OUT_DIR/mapped.bam"
  singularity exec "$SIF_DIR/samtools.sif" samtools sort -@ "$THREADS" \
    -o "$OUT_DIR/mapped.sorted.bam" "$OUT_DIR/mapped.bam"
  singularity exec "$SIF_DIR/samtools.sif" samtools index "$OUT_DIR/mapped.sorted.bam"
  singularity exec "$SIF_DIR/samtools.sif" samtools depth -a "$OUT_DIR/mapped.sorted.bam" > "$OUT_DIR/depth.txt"

  # ---------- Pilon ----------
  singularity exec "$SIF_DIR/pilon.sif" java -Xmx60G -jar /pilon/pilon.jar \
    --genome "$ASSEMBLY" \
    --frags "$OUT_DIR/mapped.sorted.bam" \
    --output "$OUT_DIR/pilon_corrected" \
    --threads "$THREADS" \
    > "$OUT_DIR/pilon.log" 2>&1
  local PILON_ASSEMBLY="$OUT_DIR/pilon_corrected.fasta"

  # ---------- Sourmash ----------
  local SM_OUT="$OUT_DIR/contamination_checks/sourmash"
  mkdir -p "$SM_OUT"
  singularity exec "$SIF_DIR/sourmash.sif" sourmash sketch dna -p k=21,scaled=1000 \
    -o "$SM_OUT/assembly.sig" "$PILON_ASSEMBLY"
  singularity exec "$SIF_DIR/sourmash.sif" sourmash gather \
    "$SM_OUT/assembly.sig" "$SM_DB" \
    -o "$SM_OUT/gather_results.csv"

  # ---------- Report (full master-style) ----------
  ln -sf "$OUT_DIR/R1_combined.fastq.gz" "$OUT_DIR/trimmed_R1.fastq.gz"
  ln -sf "$OUT_DIR/R2_combined.fastq.gz" "$OUT_DIR/trimmed_R2.fastq.gz"
  source "$SHARED_VENV/bin/activate"
  OUT_DIR="$OUT_DIR" SIF_DIR="$SIF_DIR" python3 "$COA_DIR/tools/make_sample_report.py"
  deactivate || true

  # ---------- Reorganize outputs (like master) ----------
  local RAW="$OUT_DIR/raw_outputs"
  local LOGS="$OUT_DIR/logs"
  local GRAPHS="$OUT_DIR/graphs"
  mkdir -p "$RAW/fastp" "$RAW/spades" "$RAW/kraken2" "$RAW/busco" "$RAW/mapping" "$RAW/pilon" "$RAW/sourmash" "$LOGS" "$GRAPHS"

  # raw outputs
  [ -f "$OUT_DIR/trimmed_R1.fastq.gz" ] && mv "$OUT_DIR/trimmed_R1.fastq.gz" "$RAW/fastp/"
  [ -f "$OUT_DIR/trimmed_R2.fastq.gz" ] && mv "$OUT_DIR/trimmed_R2.fastq.gz" "$RAW/fastp/"
  if [ -f "$OUT_DIR/spades_out/contigs.fasta" ]; then
    mv "$OUT_DIR/spades_out/contigs.fasta" "$RAW/spades/contigs.fasta"
    mv "$OUT_DIR/spades_out" "$RAW/spades/run_spades_out"
  fi
  [ -f "$OUT_DIR/kraken_report.txt" ] && mv "$OUT_DIR/kraken_report.txt" "$RAW/kraken2/"
  if compgen -G "$OUT_DIR/run_busco_*" > /dev/null; then
    mv "$OUT_DIR"/run_busco_* "$RAW/busco/" || true
  fi
  [ -f "$OUT_DIR/mapped.bam" ]            && mv "$OUT_DIR/mapped.bam"            "$RAW/mapping/"
  [ -f "$OUT_DIR/mapped.sorted.bam" ]     && mv "$OUT_DIR/mapped.sorted.bam"     "$RAW/mapping/"
  [ -f "$OUT_DIR/mapped.sorted.bam.bai" ] && mv "$OUT_DIR/mapped.sorted.bam.bai" "$RAW/mapping/"
  [ -f "$OUT_DIR/depth.txt" ]             && mv "$OUT_DIR/depth.txt"             "$RAW/mapping/"
  [ -f "$OUT_DIR/pilon_corrected.fasta" ] && mv "$OUT_DIR/pilon_corrected.fasta" "$RAW/pilon/"
  if [ -d "$OUT_DIR/contamination_checks/sourmash" ]; then
    mv "$OUT_DIR/contamination_checks/sourmash" "$RAW/sourmash/"
  fi

  # logs
  [ -f "$OUT_DIR/spades.log" ]         && mv "$OUT_DIR/spades.log" "$LOGS/"
  [ -f "$OUT_DIR/kraken_output.txt" ]  && mv "$OUT_DIR/kraken_output.txt" "$LOGS/"
  if compgen -G "$OUT_DIR/busco_*.log" > /dev/null; then
    mv "$OUT_DIR"/busco_*.log "$LOGS/" || true
  fi
  [ -f "$OUT_DIR/pilon.log" ]          && mv "$OUT_DIR/pilon.log" "$LOGS/"

  # graphs (leave sample_report.pdf and summary_panel.pdf in OUT_DIR root)
  for f in "$OUT_DIR"/fig_*.png "$OUT_DIR"/summary_plots.pdf; do
    [ -e "$f" ] && mv "$f" "$GRAPHS/"
  done
  [ -f "$OUT_DIR/summary_page.pdf" ] && rm -f "$OUT_DIR/summary_page.pdf" || true

  # Copy final report to parent label dir
  [ -f "$OUT_DIR/sample_report.pdf" ] && cp -f "$OUT_DIR/sample_report.pdf" "$SEQ_DIR/$LABEL/${LABEL}_report.pdf"
}

# =========================
# Step 4: Run sequential co-assemblies (2→6)
# =========================
run_seq_coassembly 2 double
run_seq_coassembly 3 triple
run_seq_coassembly 4 quadruple
run_seq_coassembly 5 pentuple
run_seq_coassembly 6 sextuple

# =========================
# Step 5: BUSCO completeness trend plot (Complete + Fragmented)
# =========================
source "$SHARED_VENV/bin/activate"
python3 - << 'PY_TREND'
import os, re, glob
import numpy as np
import matplotlib.pyplot as plt

OUT_ROOT = os.path.expanduser(os.environ.get("OUT_ROOT","~/Enhanced_Pipeline_10_out"))
COA_DIR  = os.path.join(OUT_ROOT, "co-assembly")
SEQ_DIR  = os.path.join(COA_DIR, "sequential_assemblies")

labels = [("double",2),("triple",3),("quadruple",4),("pentuple",5),("sextuple",6)]

x = []
complete = []
fragmented = []

for lbl,k in labels:
    outdir = os.path.join(SEQ_DIR,lbl)
    logs = sorted(
        glob.glob(os.path.join(outdir,"logs","busco_*.log")) +
        glob.glob(os.path.join(outdir,"busco_*.log")),
        key=os.path.getmtime, reverse=True
    )
    comp, frag = np.nan, np.nan
    for lg in logs:
        for line in open(lg, encoding='utf-8', errors='ignore'):
            m = re.search(r"C:(\d+\.\d+)%.*F:(\d+\.\d+)%", line)
            if m:
                comp, frag = float(m.group(1)), float(m.group(2))
                break
        if not np.isnan(comp): break
    x.append(k)
    complete.append(comp)
    fragmented.append(frag)

blue = "#0072BC"
yellow = "#FFC740"

plt.figure(figsize=(8,5))
plt.scatter(x, complete, color=blue, label='Complete', s=100, marker='o')
plt.plot(x, complete, color=blue, linestyle='--', alpha=0.7)
for xi, yi in zip(x, complete):
    if not np.isnan(yi):
        plt.text(xi, yi + 1, f"{yi:.1f}%", ha='center', va='bottom', fontsize=9, color='black')

plt.scatter(x, fragmented, color=yellow, label='Fragmented', s=100, marker='^')
plt.plot(x, fragmented, color=yellow, linestyle='--', alpha=0.7)
for xi, yi in zip(x, fragmented):
    if not np.isnan(yi):
        plt.text(xi, yi - 2, f"{yi:.1f}%", ha='center', va='top', fontsize=9, color='black')

plt.xticks(x, ["Double","Triple","Quadruple","Pentuple","Sextuple"])
plt.xlabel("Number of SAGs in co-assembly")
plt.ylabel("BUSCO %")
plt.title("BUSCO completeness across co-assemblies")
plt.ylim(0, 105)
plt.grid(axis='y', linestyle='--', alpha=0.5)
plt.legend()
plt.tight_layout()

out_png = os.path.join(COA_DIR, "sequential_results.png")
plt.savefig(out_png, dpi=300, bbox_inches="tight")
print(f"Saved BUSCO line plot: {out_png}")
PY_TREND
deactivate || true

echo "Sequential co-assembly workflow complete."




